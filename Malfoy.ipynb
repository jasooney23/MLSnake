{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('MLSnake': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b1af4f528c17cb738e64976452e28554f106c14c6d97ebb03ebf248c4117de78"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import queue, random, threading, math, time, pickle\n",
    "import snake_one as snake\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tkinter as tkinter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.autograph.set_verbosity(0)\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    pass\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "save_path = \"./save\"\n",
    "model_path = \"./model\"\n",
    "print(physical_devices)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4\n",
    "epsilon = 0.1\n",
    "discount = 0.99\n",
    "learning_rate = 0.0001\n",
    "memory_size = 100000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    # Q-Network\n",
    "    q1 = tf.keras.Sequential()\n",
    "\n",
    "    input_size = (15, 15, stack_size, 3)\n",
    "\n",
    "    q1.add(tf.keras.layers.Conv3D(15, (5, 5, 3),\n",
    "                                activation=\"relu\", input_shape=input_size))\n",
    "    q1.add(tf.keras.layers.Conv3D(11, (3, 3, 2),\n",
    "                                activation=\"relu\"))\n",
    "    q1.add(tf.keras.layers.Conv3D(9, (3, 3, 1),\n",
    "                                activation=\"relu\"))\n",
    "\n",
    "    q1.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    q1.add(tf.keras.layers.Dense(27, activation=\"relu\"))\n",
    "    q1.add(tf.keras.layers.Dense(5))\n",
    "\n",
    "    q1.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate), loss=\"mse\")\n",
    "    return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    q1 = tf.keras.models.load_model(save_path + \"/model1\")\n",
    "    q2 = tf.keras.models.load_model(save_path + \"/model2\")\n",
    "\n",
    "    print(\"Loaded models\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    q1 = make_model()\n",
    "    q2 = make_model()\n",
    "    print(\"Created models\")\n",
    "\n",
    "q1.summary()\n",
    "q2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(save_path + \"/update_index.dat\", \"rb\") as openfile:\n",
    "        update_index = int(pickle.load(openfile))\n",
    "    with open(save_path + \"/filled_memory.dat\", \"rb\") as openfile:\n",
    "        filled_memory = int(pickle.load(openfile))\n",
    "    with open(save_path + \"/states_memory.dat\", \"rb\") as openfile:\n",
    "        states_memory = pickle.load(openfile)\n",
    "    with open(save_path + \"/action_memory.dat\", \"rb\") as openfile:\n",
    "        action_memory = pickle.load(openfile)\n",
    "    with open(save_path + \"/reward_memory.dat\", \"rb\") as openfile:\n",
    "        reward_memory = pickle.load(openfile)\n",
    "    with open(save_path + \"/transitions_memory.dat\", \"rb\") as openfile:\n",
    "        transitions_memory = pickle.load(openfile)\n",
    "\n",
    "    with open(save_path + \"/scores.dat\", \"rb\") as openfile:\n",
    "        scores = pickle.load(openfile)\n",
    "    with open(save_path + \"/optimal_value.dat\", \"rb\") as openfile:\n",
    "        optimal_value = pickle.load(openfile)\n",
    "    print(\"Loaded replay memory\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    update_index = 0\n",
    "    filled_memory = 0\n",
    "    # Replay Memory\n",
    "    states_memory = np.ndarray((memory_size, 15, 15, stack_size, 3))\n",
    "    action_memory = np.ndarray((memory_size))\n",
    "    reward_memory = np.ndarray((memory_size))\n",
    "    transitions_memory = np.ndarray((memory_size, 15, 15, stack_size, 3))\n",
    "    scores = []\n",
    "    optimal_value = []\n",
    "    print(\"Created replay memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class experience:\n",
    "#     states = None\n",
    "#     action = None\n",
    "#     reward = None\n",
    "#     transitions = None\n",
    "    \n",
    "#     def __init__(self, states, action, reward, transitions):\n",
    "#         self.states = states\n",
    "#         self.action = action\n",
    "#         self.reward = reward\n",
    "#         self.transitions = transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    directions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"NONE\"]\n",
    "\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "        self.phi = queue.deque()\n",
    "\n",
    "        self.max_q = 0\n",
    "        self.min_q = 0\n",
    "\n",
    "    def update_memory(self, states, action, reward, transitions):\n",
    "        global update_index, filled_memory\n",
    "        if update_index >= memory_size:\n",
    "            update_index = 0\n",
    "\n",
    "        states_memory[update_index] = states\n",
    "        action_memory[update_index] = action\n",
    "        reward_memory[update_index] = reward\n",
    "        transitions_memory[update_index] = transitions\n",
    "\n",
    "        update_index += 1\n",
    "        if filled_memory < batch_size:\n",
    "            filled_memory += 1\n",
    "\n",
    "    def stack(self, frames):\n",
    "\n",
    "        fstack = np.stack(frames, axis=2)\n",
    "        # fstack = frames[0]\n",
    "        # for x in range(1, len(frames)):\n",
    "        #     frame = np.expand_dims(frames[x], axis=2)\n",
    "        #     print(fstack.shape)\n",
    "        #     print(frame.shape)\n",
    "        #     # frame = frames[x]\n",
    "        #     fstack = np.stack((fstack, frame), axis=2)\n",
    "            \n",
    "        return fstack\n",
    "\n",
    "    def epsilon_action(self):\n",
    "        # print(len(self.phi))\n",
    "        stack = np.expand_dims(self.stack(self.phi), axis=0)\n",
    "\n",
    "        if random.uniform(0, 1) <= epsilon or len(self.phi) < stack_size:\n",
    "            action = self.directions[random.randint(0, 4)]\n",
    "\n",
    "            if random.uniform(0, 1) <= 0.5:\n",
    "                self.q_selector = 1\n",
    "            else:\n",
    "                self.q_selector = 2\n",
    "\n",
    "            # print(\"random\")\n",
    "        else:\n",
    "            if random.uniform(0, 1) <= 0.5:\n",
    "                self.q_selector = 1\n",
    "                prediction = q1.predict(stack)\n",
    "            else:\n",
    "                self.q_selector = 2\n",
    "                prediction = q2.predict(stack)\n",
    "\n",
    "            maxq = np.argmax(prediction)\n",
    "            action = self.directions[maxq]\n",
    "            # print(prediction)\n",
    "            # print(action)\n",
    "            # print(prediction[0][maxq])\n",
    "        return action\n",
    "\n",
    "    def step(self):\n",
    "        if len(self.phi) == 0:\n",
    "            for x in range(stack_size):\n",
    "                self.phi.append(self.game.get_state())\n",
    "\n",
    "        action = self.epsilon_action()\n",
    "        state_reward = self.game.step(action)\n",
    "\n",
    "        phi_last = list(self.phi)\n",
    "        self.phi.append(state_reward[0])\n",
    "\n",
    "        if len(self.phi) > stack_size:\n",
    "            phi_last = self.stack(phi_last)\n",
    "            self.phi.popleft()\n",
    "            phi_current = self.stack(self.phi)\n",
    "\n",
    "            self.update_memory(phi_last, self.directions.index(action), state_reward[1], phi_current)\n",
    "\n",
    "    def get_batch_indices(self, memory):\n",
    "        indices = []\n",
    "        for x in range(batch_size):\n",
    "            indices.append(random.randint(0, filled_memory - 1))\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def losses(self):\n",
    "        yj_tensor = np.ndarray((batch_size))\n",
    "\n",
    "        indices = self.get_batch_indices(states_memory)\n",
    "        states = states_memory[indices]\n",
    "        action = action_memory[indices]\n",
    "        reward = reward_memory[indices]\n",
    "        transitions = transitions_memory[indices]\n",
    "\n",
    "        if self.q_selector == 1:\n",
    "            q_phi_next = q2.predict(transitions)\n",
    "        else:\n",
    "            q_phi_next = q1.predict(transitions)\n",
    "\n",
    "        for t in range(batch_size):\n",
    "            if reward[t] < 0:\n",
    "                yj = reward[t]\n",
    "            else:\n",
    "                yj = reward[t] + (discount * np.amax(q_phi_next[t]))\n",
    "            yj_tensor[t] = yj\n",
    "\n",
    "        return states, yj_tensor\n",
    "\n",
    "    def learn(self):\n",
    "        if filled_memory >= batch_size:\n",
    "            state_data, expected_data = self.losses()\n",
    "\n",
    "        #     before1 = q1.predict(np.expand_dims(self.stack(self.phi), axis=0))\n",
    "        #     before2 = q1.predict(np.expand_dims(self.stack(self.phi), axis=0))\n",
    "            # before1 = q1.get_weights()[0][0]\n",
    "            # before2 = q2.get_weights()[0][0]\n",
    "\n",
    "            if self.q_selector == 1:\n",
    "                q1.train_on_batch(state_data, expected_data)\n",
    "            else:\n",
    "                q2.train_on_batch(state_data, expected_data)\n",
    "\n",
    "            # gradient = tape.gradient()\n",
    "            # optimizer.apply_gradients(zip(self.grad(), model.trainable_variables))\n",
    "            # gradient = optimizer.get_gradients(losses, q.trainable_variables)\n",
    "            # optimizer.apply_gradients(gradient)\n",
    "\n",
    "            after1 = q1.predict(np.expand_dims(self.stack(self.phi), axis=0))\n",
    "            # after2 = q1.predict(np.expand_dims(self.stack(self.phi), axis=0))\n",
    "            # # # after1 = q1.get_weights()[0][0]\n",
    "            # # # after2 = q2.get_weights()[0][0]\n",
    "\n",
    "            if math.isnan(after1[0][0]):\n",
    "                print(\"NaN\")\n",
    "\n",
    "            # print(\"Before 1: \" + str(before1))\n",
    "            # print(\"After 1:  \" + str(after1) + \"\\n--------------------------------\")\n",
    "            # print(\"Before 2: \" + str(before2))\n",
    "            # print(\"After 2:  \" + str(after2) + \"\\n================================\")\n",
    "\n",
    "            if np.amax(after1) > self.max_q:\n",
    "                self.max_q = np.amax(after1)\n",
    "            if np.amin(after1) > self.min_q:\n",
    "                self.min_q = np.amin(after1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    # print(\"==============================\")\n",
    "    # print(\"AVERAGE LOSS v. FRAME\")\n",
    "    # plt.plot(average_loss)\n",
    "    # plt.xlabel(\"Frame\")\n",
    "    # plt.ylabel(\"Average Loss\")\n",
    "    # plt.show()\n",
    "    # print(\"==============================\")\n",
    "    print(\"SCORE v. EPISODE\")\n",
    "    plt.plot(scores)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.show()\n",
    "    print(\"Q RANGE v. EPISODE\")\n",
    "    plt.plot(optimal_value)\n",
    "    plt.xlabel(\"EPISODE\")\n",
    "    plt.ylabel(\"Range of Q values\")\n",
    "    plt.show()\n",
    "    print(\"==============================\")"
   ]
  },
  {
   "source": [
    "game = snake.game()\n",
    "x = 0\n",
    "\n",
    "while True:\n",
    "    # print(\"Training agent on episode \" + str(x))\n",
    "    dqn = agent(game)\n",
    "\n",
    "    game.start(dqn)\n",
    "\n",
    "    if x % 10 == 0:\n",
    "        q1.save(save_path + \"/model1\", overwrite=True, include_optimizer=True)\n",
    "        q2.save(save_path + \"/model2\", overwrite=True, include_optimizer=True)\n",
    "        q1.save(model_path + \"1\", overwrite=True, include_optimizer=True)\n",
    "        q2.save(model_path + \"2\", overwrite=True, include_optimizer=True)\n",
    "        with open(save_path + \"/update_index.dat\", \"wb\") as openfile:\n",
    "            pickle.dump(update_index, openfile)\n",
    "        with open(save_path + \"/filled_memory.dat\", \"wb\") as openfile:\n",
    "            pickle.dump(filled_memory, openfile)\n",
    "        with open(save_path + \"/states_memory.dat\", \"wb\") as openfile:\n",
    "            pickle.dump(states_memory, openfile)\n",
    "        with open(save_path + \"/action_memory.dat\", \"wb\") as openfile:\n",
    "            pickle.dump(action_memory, openfile)\n",
    "        with open(save_path + \"/reward_memory.dat\", \"wb\") as openfile:\n",
    "            pickle.dump(reward_memory, openfile)\n",
    "        with open(save_path + \"/transitions_memory.dat\", \"wb\") as openfile:\n",
    "            pickle.dump(transitions_memory, openfile)\n",
    "\n",
    "        with open(save_path + \"/scores.dat\", \"wb\") as openfile:\n",
    "            pickle.dump(scores, openfile)\n",
    "        with open(save_path + \"/optimal_value.dat\", \"wb\") as openfile:\n",
    "            pickle.dump(optimal_value, openfile)\n",
    "\n",
    "        plot()\n",
    "    x += 1\n",
    "\n",
    "    # print(\"Finished episode \" + str(x) + \", agent scored \" + str(game.score) + \" points.\")\n",
    "    scores.append(game.score)\n",
    "    optimal_value.append(dqn.max_q - dqn.min_q)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}