{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('MLSnake': conda)",
   "display_name": "Python 3.7.9 64-bit ('MLSnake': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b1af4f528c17cb738e64976452e28554f106c14c6d97ebb03ebf248c4117de78"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import queue, random, threading, math, time, pickle\n",
    "import snake_one as snake\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tkinter as tkinter\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "save_path = \"./save\"\n",
    "print(physical_devices)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 8\n",
    "epsilon = 0.1\n",
    "discount = 0.95\n",
    "learning_rate = 0.1\n",
    "memory_size = 100000\n",
    "batch_size = 1000\n",
    "\n",
    "update_index = 0\n",
    "filled_memory = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 12, 12, 15)        1935      \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 10, 10, 12)        1632      \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 9, 9, 10)          490       \n_________________________________________________________________\nflatten (Flatten)            (None, 810)               0         \n_________________________________________________________________\ndense (Dense)                (None, 108)               87588     \n_________________________________________________________________\ndense_1 (Dense)              (None, 48)                5232      \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 196       \n=================================================================\nTotal params: 97,073\nTrainable params: 97,073\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(save_path + \"/optimizer.dat\", \"rb\") as openfile:\n",
    "    optimizer = pickle.load(openfile)\n",
    "# except:\n",
    "#     optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "q = tf.keras.models.load_model(save_path + \"/model\")\n",
    "# except:\n",
    "#     # Q-Network\n",
    "#     q = tf.keras.Sequential()\n",
    "\n",
    "#     input_size = (15, 15, stack_size)\n",
    "\n",
    "#     q.add(tf.keras.layers.Conv2D(15, 3,\n",
    "#                                 activation=\"relu\", input_shape=input_size))\n",
    "#     q.add(tf.keras.layers.Conv2D(30, 3,\n",
    "#                                 activation=\"relu\"))\n",
    "#     q.add(tf.keras.layers.Conv2D(30, 3,\n",
    "#                                 activation=\"relu\"))\n",
    "#     q.add(tf.keras.layers.Flatten())\n",
    "#     q.add(tf.keras.layers.Dense(60, activation=\"relu\"))\n",
    "#     q.add(tf.keras.layers.Dense(28, activation=\"relu\"))\n",
    "#     q.add(tf.keras.layers.Dense(4))\n",
    "#     q.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "q.summary()\n",
    "\n",
    "# Replay Memory\n",
    "states_memory = np.ndarray((memory_size, 15, 15, stack_size))\n",
    "action_memory = np.ndarray((memory_size))\n",
    "reward_memory = np.ndarray((memory_size))\n",
    "transitions_memory = np.ndarray((memory_size, 15, 15, stack_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class experience:\n",
    "#     states = None\n",
    "#     action = None\n",
    "#     reward = None\n",
    "#     transitions = None\n",
    "    \n",
    "#     def __init__(self, states, action, reward, transitions):\n",
    "#         self.states = states\n",
    "#         self.action = action\n",
    "#         self.reward = reward\n",
    "#         self.transitions = transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "phi = queue.deque()\n",
    "\n",
    "def update_memory(states, action, reward, transitions):\n",
    "    global update_index, filled_memory\n",
    "    if update_index >= memory_size:\n",
    "        update_index = 0\n",
    "\n",
    "    states_memory[update_index] = states\n",
    "    action_memory[update_index] = action\n",
    "    reward_memory[update_index] = reward\n",
    "    transitions_memory[update_index] = transitions\n",
    "\n",
    "    update_index += 1\n",
    "    if filled_memory < batch_size:\n",
    "        filled_memory += 1\n",
    "\n",
    "def stack(frames):\n",
    "    fstack = frames[0]\n",
    "    for x in range(1, len(frames)):\n",
    "        fstack = np.dstack((fstack, frames[x]))\n",
    "        \n",
    "    return fstack\n",
    "\n",
    "def epsilon_action():\n",
    "    if random.uniform(0, 1) <= epsilon or len(phi) < stack_size:\n",
    "        action = directions[random.randint(0, 3)]\n",
    "    else:\n",
    "        action = directions[\n",
    "            np.argmax(\n",
    "                q.predict(\n",
    "                    np.expand_dims(stack(phi), axis=0)))]\n",
    "    return action\n",
    "\n",
    "def step():\n",
    "    action = epsilon_action()\n",
    "    game.step(action)\n",
    "\n",
    "    while return_queue.empty():\n",
    "        if not game.running:\n",
    "            break\n",
    "\n",
    "    state_reward = return_queue.get()\n",
    "\n",
    "    phi_last = list(phi)\n",
    "    phi.appendleft(state_reward[0])\n",
    "\n",
    "    if len(phi) > stack_size:\n",
    "        phi_last = stack(phi_last)\n",
    "        phi.pop()\n",
    "        phi_current = stack(phi)\n",
    "\n",
    "        update_memory(phi_last, directions.index(action), state_reward[1], phi_current)\n",
    "\n",
    "def get_batch_indices(memory):\n",
    "    indices = []\n",
    "    for x in range(batch_size):\n",
    "        indices.append(random.randint(0, filled_memory - 1))\n",
    "\n",
    "    return indices\n",
    "\n",
    "def losses():\n",
    "    loss_tensor = np.ndarray((batch_size))\n",
    "\n",
    "    indices = get_batch_indices(states_memory)\n",
    "    states = states_memory[indices]\n",
    "    action = action_memory[indices]\n",
    "    reward = reward_memory[indices]\n",
    "    transitions = transitions_memory[indices]\n",
    "\n",
    "    q_phi = q.predict(states)\n",
    "    q_phi_next = q.predict(transitions)\n",
    "\n",
    "    for t in range(batch_size):\n",
    "        yj = reward[t] + discount * np.amax(q_phi_next[t])\n",
    "        loss_tensor[t] = math.pow(yj - q_phi[t][int(action[t])], 2)\n",
    "\n",
    "    return loss_tensor\n",
    "\n",
    "def learn():\n",
    "    if np.count_nonzero(states_memory) != 0:\n",
    "        loss_batch = losses()\n",
    "\n",
    "        gradient = optimizer.get_gradients(loss_batch, ())\n",
    "        optimizer.apply_gradients(gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(epoch, snake_game):\n",
    "    time.sleep(0.01)\n",
    "    print(\"Training episode \" + str(epoch) + \".\")\n",
    "\n",
    "    while game.running:\n",
    "        step()\n",
    "        learn()\n",
    "\n",
    "    print(\"Training ended, agent scored \" + str(game.score) + \" points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training episode 0.\n",
      "Training ended, agent scored 0 points.\n",
      "WARNING:tensorflow:From C:\\Users\\jason\\anaconda3\\envs\\MLSnake\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\jason\\anaconda3\\envs\\MLSnake\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./save/model\\assets\n"
     ]
    }
   ],
   "source": [
    "game = snake.game(queue.Queue(1))\n",
    "for x in range(1):\n",
    "    return_queue = queue.Queue(1)\n",
    "\n",
    "    training_thread = threading.Thread(target=train_agent, args=(x, game)) \n",
    "    training_thread.start()\n",
    "\n",
    "    game.start(return_queue)\n",
    "    training_thread.join()\n",
    "\n",
    "    q.save(save_path + \"/model\", overwrite=True, include_optimizer=True)\n",
    "    with open(save_path + \"/optimizer.dat\", \"wb\") as openfile:\n",
    "        pickle.dump(optimizer, openfile)\n",
    "\n",
    "game.w.destroy()"
   ]
  },
  {
   "source": [
    "indices = []\n",
    "for x in range(batch_size):\n",
    "    indices.append(random.randint(0, filled_memory - 1))\n",
    "for x in range(2):\n",
    "    q.predict(states_memory[indices])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}