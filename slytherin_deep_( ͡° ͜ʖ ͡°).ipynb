{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "49706f3ca798516db12f04abd8c0ffffab6d9e72f5eedd17ba42e6f4ec51dd7d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snake, queue, random, threading, math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience:\n",
    "    states = None\n",
    "    action = None\n",
    "    reward = None\n",
    "    transitions = None\n",
    "    \n",
    "    def __init__(self, states, action, reward, transitions):\n",
    "        self.states = states\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.transitions = transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Network\n",
    "q = tf.keras.Sequential()\n",
    "q.add(tf.keras.layers.Conv2D(15, (4, 4),\n",
    "                             activation=\"relu\", input_shape=(15, 15, stack_size)))\n",
    "q.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
    "q.add(tf.keras.layers.Dense(4, activation=\"relu\"))\n",
    "q.compile(optimizer=\"Adam\", loss=\"mse\")\n",
    "\n",
    "# Replay Memory\n",
    "replay_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    directions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "    phi = queue.deque()\n",
    "\n",
    "    def __init__(self, game, return_queue, replay_memory, q, epsilon, discount, rate, batch_size):\n",
    "        self.game = game\n",
    "        self.return_queue = return_queue\n",
    "        self.replay_memory = replay_memory\n",
    "        self.q = q\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.rate = rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def epsilon_action(self):\n",
    "        if random.randrange(0, 1) <= self.epsilon:\n",
    "            action = self.directions[random.randint(0, 3)]\n",
    "        else:\n",
    "            action = self.directions[np.argmax(q.predict(self.phi))]\n",
    "        return action\n",
    "\n",
    "    def step(self):\n",
    "        action = self.epsilon_action()\n",
    "        self.game.step(action)\n",
    "\n",
    "        while self.return_queue.empty():\n",
    "            if not self.game.running:\n",
    "                break\n",
    "\n",
    "        state_reward = return_queue.get()\n",
    "\n",
    "        phi_last = np.array(self.phi)\n",
    "        self.phi.appendleft(state_reward[0])\n",
    "\n",
    "        phi_current = np.array(self.phi)\n",
    "\n",
    "        if len(self.phi) >= stack_size:\n",
    "            self.phi.pop()\n",
    "            self.replay_memory.append(experience(\n",
    "                phi_last, action, state_reward[1], phi_current))\n",
    "\n",
    "    def get_batch(self):\n",
    "        batch = np.empty([self.batch_size], dtype=experience)\n",
    "        if len(replay_memory) != 0:\n",
    "            for x in range(self.batch_size):\n",
    "                batch[x] = replay_memory[random.randint(0, len(replay_memory) - 1)].states\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def loss(self, phi, reward, action):\n",
    "        q_of_phi = self.q.predict(phi)\n",
    "        yj = reward + self.discount * np.amax(q_of_phi)\n",
    "        return math.pow(yj - q_of_phi[self.directions.index(action)], 2)\n",
    "\n",
    "    def learn(self):\n",
    "        # This probably isn't correct but might as well try it\n",
    "        if len(replay_memory) != 0:\n",
    "            batch = self.get_batch()\n",
    "            print(type(batch))\n",
    "            q.train_on_batch(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, epoch):\n",
    "    print(\"Training epoch \" + str(epoch) + \".\")\n",
    "\n",
    "    while agent.game.running:\n",
    "        agent.step()\n",
    "        agent.learn()\n",
    "\n",
    "    print(\"Training ended, agent scored \" + str(game.score) + \" points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training epoch 0.\n",
      "<class 'numpy.ndarray'>\n",
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-ad73913acf60>\", line 6, in train_agent\n",
      "  File \"<ipython-input-5-9ed98589590f>\", line 61, in learn\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1695, in train_on_batch\n",
      "    logs = train_function(iterator)\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 696, in _initialize\n",
      "    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2855, in _get_concrete_function_internal_garbage_collected\n",
      "    graph_function, _, _ = self._maybe_define_function(args, kwargs)\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3213, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3065, in _create_graph_function\n",
      "    func_graph_module.func_graph_from_py_func(\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\n",
      "    return weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "ValueError: in user code:\n",
      "\n",
      "    /Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    /Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    /Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n",
      "        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
      "    /Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n",
      "        return self._call_for_each_replica(fn, args, kwargs)\n",
      "    /Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n",
      "        return fn(*args, **kwargs)\n",
      "    /Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    /Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:747 train_step\n",
      "        y_pred = self(x, training=True)\n",
      "    /Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:975 __call__\n",
      "        input_spec.assert_input_compatibility(self.input_spec, inputs,\n",
      "    /Users/jason/anaconda3/envs/MLSnake/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:191 assert_input_compatibility\n",
      "        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n",
      "\n",
      "    ValueError: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: [3, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game = snake.game(queue.Queue(1))\n",
    "for x in range(100):\n",
    "    return_queue = queue.Queue(1)\n",
    "\n",
    "    dqn = agent(game, return_queue, replay_memory, q, 0.1, 0.95, 0.1, 10)\n",
    "\n",
    "    training_thread = threading.Thread(target=train_agent, args=(dqn, x))\n",
    "    training_thread.start()\n",
    "\n",
    "    game.start(return_queue)\n",
    "    training_thread.join()\n",
    "\n",
    "game.w.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}