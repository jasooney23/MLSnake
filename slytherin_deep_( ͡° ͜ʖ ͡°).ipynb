{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('MLSnake': conda)",
   "display_name": "Python 3.7.7 64-bit ('MLSnake': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b1af4f528c17cb738e64976452e28554f106c14c6d97ebb03ebf248c4117de78"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snake, queue, random, threading, math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience:\n",
    "    states = None\n",
    "    action = None\n",
    "    reward = None\n",
    "    transitions = None\n",
    "    \n",
    "    def __init__(self, states, action, reward, transitions):\n",
    "        self.states = states\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.transitions = transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Network\n",
    "q = tf.keras.Sequential()\n",
    "q.add(tf.keras.layers.Conv2D(15, (4, 4), activation=\"relu\", input_shape=(15, 15, stack_size)))\n",
    "q.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
    "q.add(tf.keras.layers.Dense(4, activation=\"relu\"))\n",
    "\n",
    "# Replay Memory\n",
    "replay_memory = np.empty([2000], dtype=experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    game = None\n",
    "    replay_memory = None\n",
    "    q = None\n",
    "\n",
    "    return_queue = queue.Queue(1)\n",
    "    directions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "    phi = queue.deque()\n",
    "\n",
    "    epsilon = 0\n",
    "    discount = 0\n",
    "    rate = 0\n",
    "    batch_size = 0\n",
    "\n",
    "    def __init__(self, game, replay_memory, q, epsilon, discount, rate, batch_size):\n",
    "        self.game = game\n",
    "        self.replay_memory = replay_memory\n",
    "        self.q = q\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.rate = rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def epsilon_action(self):\n",
    "        if random.random(0, 1) <= self.epsilon:\n",
    "            action = self.directions[random.randint(0, 4)]\n",
    "        else:\n",
    "            action = self.directions[np.argmax(q.predict(phi))]\n",
    "        return action\n",
    "\n",
    "    def step(self):\n",
    "        action = epsilon_action()\n",
    "        self.game.step(action)\n",
    "\n",
    "        while self.return_queue.empty():\n",
    "            if not self.game.running:\n",
    "                break\n",
    "\n",
    "        state_reward = return_queue.get()\n",
    "\n",
    "        phi_last = np.array(phi)\n",
    "        phi.appendleft(state_reward[0])\n",
    "\n",
    "        phi_current = np.array(phi)\n",
    "\n",
    "        if len(phi) >= stack_size:\n",
    "            phi.pop()\n",
    "            replay_memory.append(experience(phi_last, action, state_reward[1], phi_current))\n",
    "\n",
    "    def get_batch(self):\n",
    "        batch = np.empty([batch_size], dtype=experience)\n",
    "        for x in range(batch_size):\n",
    "            batch[x] = replay_memory[random.randint(0, len(replay_memory))]\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    def loss(self, phi, reward, action):\n",
    "        q_of_phi = self.q.predict(phi)\n",
    "        yj = reward + self.discount * np.amax(q_of_phi)\n",
    "        return math.pow(yj - q_of_phi[self.directions.index(action)], 2)\n",
    "\n",
    "    def learn(self):\n",
    "        q.train_on_batch(get_batch())        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = snake.game(return_queue)\n",
    "\n",
    "for x in range(100):\n",
    "    return_queue = queue.Queue(1)\n",
    "\n",
    "    agent_thread = threading.Thread(target=agent, args=(game, x))\n",
    "    agent_thread.start()\n",
    "\n",
    "    game.start(return_queue)\n",
    "    agent_thread.join()\n",
    "\n",
    "game.w.destroy()"
   ]
  }
 ]
}