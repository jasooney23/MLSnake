{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('MLSnake': conda)",
   "display_name": "Python 3.7.9 64-bit ('MLSnake': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b1af4f528c17cb738e64976452e28554f106c14c6d97ebb03ebf248c4117de78"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import snake, queue, random, threading, math, time, pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tkinter as tkinter\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "save_path = \"./save\"\n",
    "print(physical_devices)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 8\n",
    "epsilon = 0.1\n",
    "discount = 0.95\n",
    "learning_rate = 0.1\n",
    "memory_size = 100000\n",
    "batch_size = 1000\n",
    "\n",
    "update_index = 0\n",
    "filled_memory = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 12, 12, 15)        1935      \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 10, 10, 12)        1632      \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 9, 9, 10)          490       \n_________________________________________________________________\nflatten (Flatten)            (None, 810)               0         \n_________________________________________________________________\ndense (Dense)                (None, 108)               87588     \n_________________________________________________________________\ndense_1 (Dense)              (None, 48)                5232      \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 196       \n=================================================================\nTotal params: 97,073\nTrainable params: 97,073\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(save_path + \"/optimizer.dat\", \"rb\") as openfile:\n",
    "        optimizer = pickle.load(openfile)\n",
    "except:\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "try:\n",
    "    q = tf.keras.models.load_model(save_path + \"/model\")\n",
    "except:\n",
    "    # Q-Network\n",
    "    q = tf.keras.Sequential()\n",
    "\n",
    "    input_size = (15, 15, stack_size)\n",
    "\n",
    "    q.add(tf.keras.layers.Conv2D(15, 3,\n",
    "                                activation=\"relu\", input_shape=input_size))\n",
    "    q.add(tf.keras.layers.Conv2D(30, 3,\n",
    "                                activation=\"relu\"))\n",
    "    q.add(tf.keras.layers.Conv2D(30, 3,\n",
    "                                activation=\"relu\"))\n",
    "    q.add(tf.keras.layers.Flatten())\n",
    "    q.add(tf.keras.layers.Dense(60, activation=\"relu\"))\n",
    "    q.add(tf.keras.layers.Dense(28, activation=\"relu\"))\n",
    "    q.add(tf.keras.layers.Dense(4))\n",
    "    q.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "q.summary()\n",
    "\n",
    "# Replay Memory\n",
    "states_memory = np.ndarray((memory_size, 15, 15, stack_size))\n",
    "action_memory = np.ndarray((memory_size))\n",
    "reward_memory = np.ndarray((memory_size))\n",
    "transitions_memory = np.ndarray((memory_size, 15, 15, stack_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class experience:\n",
    "#     states = None\n",
    "#     action = None\n",
    "#     reward = None\n",
    "#     transitions = None\n",
    "    \n",
    "#     def __init__(self, states, action, reward, transitions):\n",
    "#         self.states = states\n",
    "#         self.action = action\n",
    "#         self.reward = reward\n",
    "#         self.transitions = transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    directions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "    phi = queue.deque()\n",
    "\n",
    "    def __init__(self, game, return_queue):\n",
    "        self.game = game\n",
    "        self.return_queue = return_queue\n",
    "\n",
    "    def update_memory(self, states, action, reward, transitions):\n",
    "        global update_index, filled_memory\n",
    "        if update_index >= memory_size:\n",
    "            update_index = 0\n",
    "\n",
    "        states_memory[update_index] = states\n",
    "        action_memory[update_index] = action\n",
    "        reward_memory[update_index] = reward\n",
    "        transitions_memory[update_index] = transitions\n",
    "\n",
    "        update_index += 1\n",
    "        if filled_memory < batch_size:\n",
    "            filled_memory += 1\n",
    "\n",
    "    def stack(self, frames):\n",
    "        fstack = frames[0]\n",
    "        for x in range(1, len(frames)):\n",
    "            fstack = np.dstack((fstack, frames[x]))\n",
    "            \n",
    "        return fstack\n",
    "\n",
    "    def epsilon_action(self):\n",
    "        if random.uniform(0, 1) <= epsilon or len(self.phi) < stack_size:\n",
    "            action = self.directions[random.randint(0, 3)]\n",
    "        else:\n",
    "            action = self.directions[\n",
    "                np.argmax(\n",
    "                    q.predict(\n",
    "                        np.expand_dims(self.stack(self.phi), axis=0)))]\n",
    "        return action\n",
    "\n",
    "    def step(self):\n",
    "        action = self.epsilon_action()\n",
    "        self.game.step(action)\n",
    "\n",
    "        while self.return_queue.empty():\n",
    "            if not self.game.running:\n",
    "                break\n",
    "\n",
    "        state_reward = self.return_queue.get()\n",
    "\n",
    "        phi_last = list(self.phi)\n",
    "        self.phi.appendleft(state_reward[0])\n",
    "\n",
    "        if len(self.phi) > stack_size:\n",
    "            phi_last = self.stack(phi_last)\n",
    "            self.phi.pop()\n",
    "            phi_current = self.stack(self.phi)\n",
    "\n",
    "            self.update_memory(phi_last, self.directions.index(action), state_reward[1], phi_current)\n",
    "\n",
    "    def get_batch_indices(self, memory):\n",
    "        indices = []\n",
    "        for x in range(batch_size):\n",
    "            indices.append(random.randint(0, filled_memory - 1))\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def losses(self):\n",
    "        loss_tensor = np.ndarray((batch_size))\n",
    "\n",
    "        indices = self.get_batch_indices(states_memory)\n",
    "        states = states_memory[indices]\n",
    "        action = action_memory[indices]\n",
    "        reward = reward_memory[indices]\n",
    "        transitions = transitions_memory[indices]\n",
    "\n",
    "        print(\"before\")\n",
    "        q_phi = q.predict(states)\n",
    "        q_phi_next = q.predict(transitions)\n",
    "        print(\"after\")\n",
    "\n",
    "        for t in range(batch_size):\n",
    "            yj = reward[t] + discount * np.amax(q_phi_next[t])\n",
    "            loss_tensor[t] = math.pow(yj - q_phi[t][int(action[t])], 2)\n",
    "\n",
    "        return loss_tensor\n",
    "\n",
    "    def learn(self):\n",
    "        if np.count_nonzero(states_memory) != 0:\n",
    "            losses = self.losses()\n",
    "\n",
    "            gradient = optimizer.get_gradients(losses, ())\n",
    "            optimizer.apply_gradients(gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, epoch):\n",
    "    time.sleep(0.01)\n",
    "    print(\"Training episode \" + str(epoch) + \".\")\n",
    "\n",
    "    while agent.game.running:\n",
    "        agent.step()\n",
    "        agent.learn()\n",
    "\n",
    "    print(\"Training ended, agent scored \" + str(agent.game.score) + \" points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training episode 0.\n",
      "before\n",
      "after\n",
      "before\n",
      "after\n",
      "before\n",
      "after\n",
      "before\n",
      "after\n",
      "before\n",
      "after\n",
      "before\n",
      "after\n",
      "before\n",
      "after\n",
      "before\n",
      "after\n",
      "before\n",
      "after\n",
      "before\n",
      "after\n",
      "before\n",
      "after\n",
      "before\n",
      "after\n",
      "Training ended, agent scored 0 points.\n",
      "INFO:tensorflow:Assets written to: ./save/model\\assets\n"
     ]
    }
   ],
   "source": [
    "game = snake.game(queue.Queue(1))\n",
    "for x in range(1):\n",
    "    return_queue = queue.Queue(1)\n",
    "\n",
    "    dqn = agent(game, return_queue)\n",
    "\n",
    "    training_thread = threading.Thread(target=train_agent, args=(dqn, x)) \n",
    "    training_thread.start()\n",
    "\n",
    "    game.start(return_queue)\n",
    "    training_thread.join()\n",
    "\n",
    "    q.save(save_path + \"/model\", overwrite=True, include_optimizer=True)\n",
    "    with open(save_path + \"/optimizer.dat\", \"wb\") as openfile:\n",
    "        pickle.dump(optimizer, openfile)\n",
    "\n",
    "game.w.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for x in range(batch_size):\n",
    "    indices.append(random.randint(0, filled_memory - 1)) \n",
    "for x in range(2):\n",
    "    q.predict(states_memory[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}