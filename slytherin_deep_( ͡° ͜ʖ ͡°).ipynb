{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('MLSnake': conda)",
   "display_name": "Python 3.7.7 64-bit ('MLSnake': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b1af4f528c17cb738e64976452e28554f106c14c6d97ebb03ebf248c4117de78"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snake, queue, random, threading, math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Network\n",
    "q = tf.keras.Sequential()\n",
    "\n",
    "input_1 = (stack_size, 15, 15, 1)\n",
    "input_2 = (stack_size, 5, 5, 1)\n",
    "\n",
    "q.add(tf.keras.layers.Conv2D(15, 4,\n",
    "                             activation=\"relu\", input_shape=input_1[1:]))\n",
    "q.add(tf.keras.layers.Conv2D(5, 2,\n",
    "                             activation=\"relu\", input_shape=input_2[1:]))\n",
    "q.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
    "q.add(tf.keras.layers.Dense(4, activation=\"relu\"))\n",
    "q.compile(optimizer=\"Adam\", loss=\"mse\")\n",
    "\n",
    "# Replay Memory\n",
    "replay_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience:\n",
    "    states = None\n",
    "    action = None\n",
    "    reward = None\n",
    "    transitions = None\n",
    "    \n",
    "    def __init__(self, states, action, reward, transitions):\n",
    "        self.states = states\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.transitions = transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    directions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "    phi = queue.deque()\n",
    "\n",
    "    def __init__(self, game, return_queue, replay_memory, q, epsilon, discount, rate, batch_size):\n",
    "        self.game = game\n",
    "        self.return_queue = return_queue\n",
    "        self.replay_memory = replay_memory\n",
    "        self.q = q\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.rate = rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def epsilon_action(self):\n",
    "        if random.randrange(0, 1) <= self.epsilon:\n",
    "            action = self.directions[random.randint(0, 3)]\n",
    "        else:\n",
    "            action = self.directions[np.argmax(q.predict(self.phi))]\n",
    "        return action\n",
    "\n",
    "    def step(self):\n",
    "        action = self.epsilon_action()\n",
    "        self.game.step(action)\n",
    "\n",
    "        while self.return_queue.empty():\n",
    "            if not self.game.running:\n",
    "                break\n",
    "\n",
    "        state_reward = return_queue.get()\n",
    "\n",
    "        phi_last = np.array(self.phi)\n",
    "        self.phi.appendleft(np.expand_dims(state_reward[0], axis=2))\n",
    "\n",
    "        phi_current = np.array(self.phi)\n",
    "\n",
    "        if len(self.phi) >= stack_size:\n",
    "            self.phi.pop()\n",
    "            self.replay_memory.append(experience(\n",
    "                phi_last, action, state_reward[1], phi_current))\n",
    "\n",
    "    def get_batch(self):\n",
    "        batch = np.empty([self.batch_size], dtype=experience)\n",
    "        if len(replay_memory) != 0:\n",
    "            for x in range(self.batch_size):\n",
    "                batch[x] = replay_memory[random.randint(0, len(replay_memory) - 1)]\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def loss(self, phi, reward, action):\n",
    "        q_of_phi = self.q.predict(phi)\n",
    "        yj = reward + self.discount * np.amax(q_of_phi)\n",
    "        return math.pow(yj - q_of_phi[self.directions.index(action)], 2)\n",
    "\n",
    "    def learn(self):\n",
    "        # This probably isn't correct but might as well try it\n",
    "        if len(replay_memory) != 0:\n",
    "            batch = self.get_batch()\n",
    "            # print(type(batch))\n",
    "            # q.train_on_batch(batch)\n",
    "            print(self.loss(batch[0].states, batch[0].reward, batch[0].action))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, epoch):\n",
    "    print(\"Training epoch \" + str(epoch) + \".\")\n",
    "\n",
    "    while agent.game.running:\n",
    "        agent.step()\n",
    "        agent.learn()\n",
    "\n",
    "    print(\"Training ended, agent scored \" + str(game.score) + \" points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training epoch 0.\n"
     ]
    }
   ],
   "source": [
    "game = snake.game(queue.Queue(1))\n",
    "for x in range(100):\n",
    "    return_queue = queue.Queue(1)\n",
    "\n",
    "    dqn = agent(game, return_queue, replay_memory, q, 0.1, 0.95, 0.1, 10)\n",
    "\n",
    "    training_thread = threading.Thread(target=train_agent, args=(dqn, x))\n",
    "    training_thread.start()\n",
    "\n",
    "    game.start(return_queue)\n",
    "    training_thread.join()\n",
    "\n",
    "game.w.destroy()"
   ]
  }
 ]
}