{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('MLSnake': conda)",
   "display_name": "Python 3.8.5 64-bit ('MLSnake': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8e7db409f32daf0ebe141d6924ccb65267048167d522663b4fdc180734dfeba3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snake, queue, random, threading, math, time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4\n",
    "epsilon = 0.1\n",
    "discount = 0.95\n",
    "learning_rate = 0.1\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 12, 12, 15)        255       \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 11, 11, 5)         305       \n_________________________________________________________________\nflatten (Flatten)            (None, 605)               0         \n_________________________________________________________________\ndense (Dense)                (None, 32)                19392     \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 19,985\nTrainable params: 19,985\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "# Q-Network\n",
    "q = tf.keras.Sequential()\n",
    "\n",
    "input_1 = (stack_size, 15, 15, 1)\n",
    "input_2 = (stack_size, 5, 5, 1)\n",
    "\n",
    "q.add(tf.keras.layers.Conv2D(15, 4,\n",
    "                             activation=\"relu\", input_shape=input_1[1:]))\n",
    "q.add(tf.keras.layers.Conv2D(5, 2,\n",
    "                             activation=\"relu\", input_shape=input_2[1:]))\n",
    "q.add(tf.keras.layers.Flatten())\n",
    "q.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
    "q.add(tf.keras.layers.Dense(1, activation=\"relu\"))\n",
    "q.compile(optimizer=\"SGD\", loss=\"mse\")\n",
    "\n",
    "q.summary()\n",
    "\n",
    "# Replay Memory\n",
    "replay_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience:\n",
    "    states = None\n",
    "    action = None\n",
    "    reward = None\n",
    "    transitions = None\n",
    "    \n",
    "    def __init__(self, states, action, reward, transitions):\n",
    "        self.states = states\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.transitions = transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    directions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "    phi = queue.deque()\n",
    "\n",
    "    def __init__(self, game, return_queue, replay_memory, q, optimizer, epsilon, discount, rate, batch_size):\n",
    "        self.game = game\n",
    "        self.return_queue = return_queue\n",
    "        self.replay_memory = replay_memory\n",
    "        self.q = q\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.rate = rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def epsilon_action(self):\n",
    "        if random.randrange(0, 1) <= self.epsilon:\n",
    "            action = self.directions[random.randint(0, 3)]\n",
    "        else:\n",
    "            action = self.directions[np.argmax(q.predict(self.phi))]\n",
    "        return action\n",
    "\n",
    "    def step(self):\n",
    "        action = self.epsilon_action()\n",
    "        self.game.step(action)\n",
    "\n",
    "        while self.return_queue.empty():\n",
    "            if not self.game.running:\n",
    "                break\n",
    "\n",
    "        state_reward = return_queue.get()\n",
    "\n",
    "        phi_last = np.array(self.phi)\n",
    "        self.phi.appendleft(np.expand_dims(state_reward[0], axis=2))\n",
    "\n",
    "        phi_current = np.array(self.phi)\n",
    "\n",
    "        if len(self.phi) > stack_size:\n",
    "            self.phi.pop()\n",
    "            self.replay_memory.append(experience(\n",
    "                phi_last, action, state_reward[1], phi_current))\n",
    "\n",
    "    def get_batch(self):\n",
    "        batch = np.empty([self.batch_size], dtype=experience)\n",
    "        if len(replay_memory) != 0:\n",
    "            for x in range(self.batch_size):\n",
    "                batch[x] = replay_memory[random.randint(0, len(replay_memory) - 1)]\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def loss(self, e):\n",
    "        q_phi = self.q(e.states)\n",
    "        q_phi_next = self.q(e.transitions)\n",
    "        yj = e.reward + self.discount * np.amax(q_phi_next)\n",
    "        return math.pow(yj - q_phi[self.directions.index(e.action)], 2)\n",
    "\n",
    "    def learn(self):\n",
    "        # This probably isn't correct but might as well try it\n",
    "        if len(replay_memory) != 0:\n",
    "            batch = self.get_batch()\n",
    "            for e in batch:\n",
    "                optimizer.apply_gradients(optimizer.get_gradients(self.loss(e)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, epoch):\n",
    "    time.sleep(1)\n",
    "    print(\"Training epoch \" + str(epoch) + \".\")\n",
    "\n",
    "    while agent.game.running:\n",
    "        agent.step()\n",
    "        agent.learn()\n",
    "\n",
    "    print(\"Training ended, agent scored \" + str(game.score) + \" points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training epoch 0.\n",
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/jason/anaconda3/envs/MLSnake/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-9b0dd7c6819d>\", line 7, in train_agent\n",
      "  File \"<ipython-input-5-90d790257db2>\", line 63, in learn\n",
      "TypeError: get_gradients() missing 1 required positional argument: 'params'\n"
     ]
    }
   ],
   "source": [
    "game = snake.game(queue.Queue(1))\n",
    "for x in range(100):\n",
    "    return_queue = queue.Queue(1)\n",
    "\n",
    "    dqn = agent(game, return_queue, replay_memory, q, optimizer, epsilon, discount, learning_rate, batch_size)\n",
    "\n",
    "    training_thread = threading.Thread(target=train_agent, args=(dqn, x))\n",
    "    training_thread.start()\n",
    "\n",
    "    game.start(return_queue)\n",
    "    training_thread.join()\n",
    "\n",
    "game.w.destroy()"
   ]
  }
 ]
}