{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('MLSnake': conda)",
   "display_name": "Python 3.7.7 64-bit ('MLSnake': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b1af4f528c17cb738e64976452e28554f106c14c6d97ebb03ebf248c4117de78"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snake, queue, random, threading, math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience:\n",
    "    states = None\n",
    "    action = None\n",
    "    reward = None\n",
    "    transitions = None\n",
    "    \n",
    "    def __init__(self, states, action, reward, transitions):\n",
    "        self.states = states\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.transitions = transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Network\n",
    "q = tf.keras.Sequential()\n",
    "q.add(tf.keras.layers.Conv2D(15, (4, 4),\n",
    "                             activation=\"relu\", input_shape=(15, 15, stack_size)))\n",
    "q.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
    "q.add(tf.keras.layers.Dense(4, activation=\"relu\"))\n",
    "q.compile(optimizer=\"Adam\", loss=\"mse\")\n",
    "\n",
    "# Replay Memory\n",
    "replay_memory = np.empty([2000], dtype=experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    directions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "    phi = queue.deque()\n",
    "\n",
    "    def __init__(self, game, return_queue, replay_memory, q, epsilon, discount, rate, batch_size):\n",
    "        self.game = game\n",
    "        self.return_queue = return_queue\n",
    "        self.replay_memory = replay_memory\n",
    "        self.q = q\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.rate = rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def epsilon_action(self):\n",
    "        if random.randrange(0, 1) <= self.epsilon:\n",
    "            action = self.directions[random.randint(0, 4)]\n",
    "        else:\n",
    "            action = self.directions[np.argmax(q.predict(self.phi))]\n",
    "        return action\n",
    "\n",
    "    def step(self):\n",
    "        action = self.epsilon_action()\n",
    "        self.game.step(action)\n",
    "\n",
    "        while self.return_queue.empty():\n",
    "            if not self.game.running:\n",
    "                break\n",
    "\n",
    "        state_reward = return_queue.get()\n",
    "\n",
    "        phi_last = np.array(self.phi)\n",
    "        self.phi.appendleft(state_reward[0])\n",
    "\n",
    "        phi_current = np.array(self.phi)\n",
    "\n",
    "        if len(self.phi) >= stack_size:\n",
    "            self.phi.pop()\n",
    "            self.replay_memory.append(experience(\n",
    "                phi_last, action, state_reward[1], phi_current))\n",
    "\n",
    "    def get_batch(self):\n",
    "        batch = np.empty([self.batch_size], dtype=experience)\n",
    "        for x in range(self.batch_size):\n",
    "            batch[x] = replay_memory[random.randint(0, len(replay_memory))].states\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def loss(self, phi, reward, action):\n",
    "        q_of_phi = self.q.predict(phi)\n",
    "        yj = reward + self.discount * np.amax(q_of_phi)\n",
    "        return math.pow(yj - q_of_phi[self.directions.index(action)], 2)\n",
    "\n",
    "    def learn(self):\n",
    "        # This probably isn't correct but might as well try it\n",
    "        q.train_on_batch(self.get_batch())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, epoch):\n",
    "    print(\"Training epoch \" + str(epoch) + \".\")\n",
    "\n",
    "    while agent.game.running:\n",
    "        print(\"stepping\")\n",
    "        agent.step()\n",
    "        print(\"learning\")\n",
    "        agent.learn()\n",
    "\n",
    "    print(\"Training ended, agent scored \" + str(game.score) + \" points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training epoch 0.\n",
      "stepping\n",
      "learning\n",
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\envs\\MLSnake\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\jason\\anaconda3\\envs\\MLSnake\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-89139a0e9950>\", line 8, in train_agent\n",
      "    agent.learn()\n",
      "  File \"<ipython-input-5-ef7105934602>\", line 57, in learn\n",
      "    q.train_on_batch(self.get_batch())\n",
      "  File \"<ipython-input-5-ef7105934602>\", line 46, in get_batch\n",
      "    batch[x] = replay_memory[random.randint(0, len(replay_memory))].states\n",
      "AttributeError: 'NoneType' object has no attribute 'states'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game = snake.game(queue.Queue(1))\n",
    "for x in range(100):\n",
    "    return_queue = queue.Queue(1)\n",
    "\n",
    "    dqn = agent(game, return_queue, replay_memory, q, 0.1, 0.95, 0.1, 10)\n",
    "\n",
    "    training_thread = threading.Thread(target=train_agent, args=(dqn, x))\n",
    "    training_thread.start()\n",
    "\n",
    "    game.start(return_queue)\n",
    "    training_thread.join()\n",
    "\n",
    "game.w.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}